{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT model for text extraction with the SQuAD dataset\n",
    "\n",
    "We are going to fine-tune [BERT implemented by HuggingFace](https://huggingface.co/bert-base-uncased) for the text-extraction task with a dataset of questions and answers with the [SQuAD (The Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/) dataset.\n",
    "The data is composed by a set of questions and corresponding paragraphs that contains the answers.\n",
    "The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to do the training using multiple GPUs.\n",
    "\n",
    "This notebook is based on [BERT (from HuggingFace Transformers) for Text Extraction](https://keras.io/examples/nlp/text_extraction_with_bert/).\n",
    "\n",
    "More info:\n",
    "- [Glossary - HuggingFace docs](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [BERT NLP â€” How To Build a Question Answering Bot](https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster start -n 2 --mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import dataset_utils as du\n",
    "import eval_utils as eu\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hf_model = 'bert-base-uncased'\n",
    "bert_cache = os.path.join(os.getcwd(), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(\n",
    "    hf_model,\n",
    "    cache_dir=os.path.join(bert_cache, f'_{hf_model}-tokenizer')\n",
    ")\n",
    "save_path = os.path.join(bert_cache, f'{hf_model}-tokenizer')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    slow_tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(os.path.join(save_path, 'vocab.txt'),\n",
    "                                   lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    hf_model,\n",
    "    cache_dir=os.path.join(bert_cache, f'{hf_model}_qa')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "train_path = os.path.join(bert_cache, 'data', 'train-v1.1.json')\n",
    "eval_path = os.path.join(bert_cache, 'data', 'dev-v1.1.json')\n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "max_len = 384\n",
    "\n",
    "train_squad_examples = du.create_squad_examples(raw_train_data, max_len, tokenizer)\n",
    "x_train, y_train = du.create_inputs_targets(train_squad_examples, shuffle=True, seed=42)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = du.create_squad_examples(raw_eval_data, max_len, tokenizer)\n",
    "x_eval, y_eval = du.create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.x[0][idx]),\n",
    "                torch.tensor(self.x[1][idx]),\n",
    "                torch.tensor(self.x[2][idx]),\n",
    "                torch.tensor(self.y[0][idx]),\n",
    "                torch.tensor(self.y[1][idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from pt_distr_env import setup_distr_env\n",
    "\n",
    "setup_distr_env()\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "world_size = dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "batch_size = 16\n",
    "\n",
    "train_set = SquadDataset(x_train, y_train)\n",
    "train_sampler = DistributedSampler(train_set, num_replicas=world_size,\n",
    "                                   rank=rank, shuffle=False, seed=42)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,\n",
    "                          shuffle=False, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "device = 0\n",
    "model.to(device)\n",
    "model = DistributedDataParallel(model, device_ids=[device])\n",
    "model.train()\n",
    "\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "for epoch in range(1):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        outputs = model(input_ids=batch[0].to(device),\n",
    "                        token_type_ids=batch[1].to(device),\n",
    "                        attention_mask=batch[2].to(device),\n",
    "                        start_positions=batch[3].to(device),\n",
    "                        end_positions=batch[4].to(device)\n",
    "                       )\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "            \n",
    "        # if i > 10:\n",
    "        #      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "model_hash = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "model_path_name = './cache/model_trained_2_nodes_{model_hash}'\n",
    "\n",
    "# save model's state_dict\n",
    "# the model now is a DDP model\n",
    "# use `model.module.state_dict()` in order the load it later on\n",
    "# any number of nodes\n",
    "torch.save(model.module.state_dict(), model_path_name)\n",
    "\n",
    "# create the model again since the previous one is on the gpu\n",
    "model_cpu = BertForQuestionAnswering.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    cache_dir=os.path.join(bert_cache, 'bert-base-uncased_qa')\n",
    ")\n",
    "\n",
    "# load the model on cpu\n",
    "model_cpu.load_state_dict(\n",
    "    torch.load(model_path_name,\n",
    "               map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "# load the model on gpu\n",
    "# model.load_state_dict(torch.load(model_path_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "model.eval()\n",
    "\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "samples = np.random.choice(len(x_eval[0]), 50, replace=False)\n",
    "\n",
    "eu.EvalUtility(\n",
    "    (x_eval[0][samples], x_eval[1][samples], x_eval[2][samples]),\n",
    "    model_cpu,\n",
    "    eval_squad_examples[samples]\n",
    ").results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
